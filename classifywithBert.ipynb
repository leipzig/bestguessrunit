{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifywithBert",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leipzig/bestguessrunit/blob/main/classifywithBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OBmC3UmYaRP"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVCytM2-YWWR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "99b97851-93f6-4756-fab6-6f5036339242"
      },
      "source": [
        "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-0055acc1b68b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5L6dnEoy51r"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGQpFos2EZCB",
        "outputId": "0bdbdd40-52f2-4121-8b37-7b3ff07d7da6"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/truwl/capanno/biowdl-structs/workflows/biowdl/RNA-seq/0.1.0/rnaseq_inputs-metadata.yaml > /content/rnaseq.yaml\n",
        "!curl https://raw.githubusercontent.com/truwl/capanno/master/workflows/Broad/WholeGenomeGermlineSingleSample/v2.3.1/wgs_inputs-metadata.yaml > /content/wgs.yaml\n",
        "!curl https://raw.githubusercontent.com/truwl/capanno/master/workflows/ENCODE-DCC/atac-seq-pipeline/v1.9.0/atac-inputs-metadata.yaml > /content/atac.yaml"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  6390  100  6390    0     0  32938      0 --:--:-- --:--:-- --:--:-- 32769\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  7697  100  7697    0     0  50307      0 --:--:-- --:--:-- --:--:-- 50638\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 38523  100 38523    0     0   316k      0 --:--:-- --:--:-- --:--:--  316k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyZ65LZ8YgtG"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY4NR2cVZNFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21546d96-0072-4e25-dc61-b628169ef094"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl7rqQ4uZPiv"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import yaml\n",
        "# Preliminaries\n",
        "\n",
        "\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDBAx2Q-ZMl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac527847-9239-4f7c-aac0-bd2904e13bd4"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqOJAYCiYlZs"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV5TMDTbMPtF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5c8a67d2-f62c-449b-e15b-aaedc55097c6"
      },
      "source": [
        "def runitodf(runitpath):\n",
        "  with open(runitpath) as metaf:\n",
        "    meta=yaml.load(metaf,Loader=yaml.Loader)\n",
        "    metakeys = meta['parameter_meta']\n",
        "    df = pd.DataFrame(columns=['inputfield','desc','label'])\n",
        "    for k in metakeys:\n",
        "      df = df.append({\n",
        "      \"inputfield\": k,\n",
        "      \"desc\": metakeys[k].get('description',''),\n",
        "      \"label\": metakeys[k]['group']\n",
        "        }, ignore_index=True)\n",
        "    return(df)\n",
        "    \n",
        "rnaseq=runitodf(\"/content/rnaseq.yaml\")\n",
        "atac=runitodf(\"/content/atac.yaml\")\n",
        "wgs=runitodf(\"/content/wgs.yaml\")\n",
        "df=rnaseq.append(atac).append(wgs)\n",
        "df"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputfield</th>\n",
              "      <th>desc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RNAseq.sampleConfigFile</td>\n",
              "      <td>The sample configuration file</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RNAseq.dockerImagesFile</td>\n",
              "      <td>A file listing the used docker images</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RNAseq.starIndex</td>\n",
              "      <td>A list of star index files</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RNAseq.referenceFasta</td>\n",
              "      <td>A path to a reference fasta</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNAseq.referenceFastaFai</td>\n",
              "      <td>The path to the index associated with the refe...</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>WholeGenomeGermlineSingleSample.scatter_settings</td>\n",
              "      <td></td>\n",
              "      <td>PapiScat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>WholeGenomeGermlineSingleSample.AggregatedBamQ...</td>\n",
              "      <td>Collect GC bias metrics for aggregates</td>\n",
              "      <td>OutputOptions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>WholeGenomeGermlineSingleSample.UnmappedBamToA...</td>\n",
              "      <td>Markdups memory mulitplier</td>\n",
              "      <td>DiskAndMemory</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>WholeGenomeGermlineSingleSample.UnmappedBamToA...</td>\n",
              "      <td>Hard clip reads</td>\n",
              "      <td>CoreOptions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>WholeGenomeGermlineSingleSample.CollectRawWgsM...</td>\n",
              "      <td>Collect WGS Metrics additional disk</td>\n",
              "      <td>DiskAndMemory</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           inputfield  ...            label\n",
              "0                             RNAseq.sampleConfigFile  ...  required_inputs\n",
              "1                             RNAseq.dockerImagesFile  ...  required_inputs\n",
              "2                                    RNAseq.starIndex  ...  required_inputs\n",
              "3                               RNAseq.referenceFasta  ...  required_inputs\n",
              "4                            RNAseq.referenceFastaFai  ...  required_inputs\n",
              "..                                                ...  ...              ...\n",
              "27   WholeGenomeGermlineSingleSample.scatter_settings  ...         PapiScat\n",
              "28  WholeGenomeGermlineSingleSample.AggregatedBamQ...  ...    OutputOptions\n",
              "29  WholeGenomeGermlineSingleSample.UnmappedBamToA...  ...    DiskAndMemory\n",
              "30  WholeGenomeGermlineSingleSample.UnmappedBamToA...  ...      CoreOptions\n",
              "31  WholeGenomeGermlineSingleSample.CollectRawWgsM...  ...    DiskAndMemory\n",
              "\n",
              "[188 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF1DCVrCh6_d"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdGccqrAZYjw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "85184ff2-f435-4e7d-d08f-96fafa50b9fd"
      },
      "source": [
        "# Model parameter\n",
        "MAX_SEQ_LEN = 10\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "import numpy as np\n",
        "# Fields\n",
        "\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "fields = [('label', label_field), ('desc', text_field),('inputfield',text_field)]\n",
        "# TabularDataset\n",
        "\n",
        "#https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n",
        "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
        "    np.random.seed(seed)\n",
        "    perm = np.random.permutation(df.index)\n",
        "    m = len(df.index)\n",
        "    train_end = int(train_percent * m)\n",
        "    validate_end = int(validate_percent * m) + train_end\n",
        "    train = df.iloc[perm[:train_end]]\n",
        "    validate = df.iloc[perm[train_end:validate_end]]\n",
        "    test = df.iloc[perm[validate_end:]]\n",
        "    return train, validate, test\n",
        "\n",
        "traindf, validatedf, testdf = train_validate_test_split(df)\n",
        "traindf.to_csv(path_or_buf=\"/content/train.csv\", sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict')\n",
        "validatedf.to_csv(path_or_buf=\"/content/validate.csv\", sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict')\n",
        "testdf.to_csv(path_or_buf=\"/content/test.csv\", sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict')\n",
        "\n",
        "train, valid, test = TabularDataset.splits(path=\"/content/\", train='train.csv', validation='validate.csv',\n",
        "                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "# Iterators\n",
        "train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.inputfield),\n",
        "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
        "valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.inputfield),\n",
        "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
        "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)\n",
        "traindf"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputfield</th>\n",
              "      <th>desc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>RNAseq.umiDeduplication</td>\n",
              "      <td>Whether or not UMI based deduplication should ...</td>\n",
              "      <td>optional_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RNAseq.dockerImagesFile</td>\n",
              "      <td>A file listing the used docker images</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>RNAseq.lncRNAdetection</td>\n",
              "      <td>Whether or not lncRNA detection should be perf...</td>\n",
              "      <td>lncRNA_detection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>RNAseq.hisat2Index</td>\n",
              "      <td>use hisat2</td>\n",
              "      <td>hisat2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>atac.prom</td>\n",
              "      <td>Promoter regions file in BED format.</td>\n",
              "      <td>reference_genome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RNAseq.sampleConfigFile</td>\n",
              "      <td>The sample configuration file</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RNAseq.referenceFasta</td>\n",
              "      <td>A path to a reference fasta</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNAseq.referenceFastaFai</td>\n",
              "      <td>The path to the index associated with the refe...</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RNAseq.referenceFastaDict</td>\n",
              "      <td>The path to the dict file associated with the ...</td>\n",
              "      <td>required_inputs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>atac.enable_compare_to_roadmap</td>\n",
              "      <td>Enables comparison to Roadmap.</td>\n",
              "      <td>pipeline_parameter</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>112 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        inputfield  ...               label\n",
              "11         RNAseq.umiDeduplication  ...     optional_inputs\n",
              "1          RNAseq.dockerImagesFile  ...     required_inputs\n",
              "19          RNAseq.lncRNAdetection  ...    lncRNA_detection\n",
              "20              RNAseq.hisat2Index  ...              hisat2\n",
              "16                       atac.prom  ...    reference_genome\n",
              "..                             ...  ...                 ...\n",
              "0          RNAseq.sampleConfigFile  ...     required_inputs\n",
              "3            RNAseq.referenceFasta  ...     required_inputs\n",
              "4         RNAseq.referenceFastaFai  ...     required_inputs\n",
              "5        RNAseq.referenceFastaDict  ...     required_inputs\n",
              "63  atac.enable_compare_to_roadmap  ...  pipeline_parameter\n",
              "\n",
              "[112 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-lWIMaYnsA"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RkcXCHSph1_"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-uncased\"\n",
        "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
        "\n",
        "        return loss, text_fea"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z81slSELYqO1"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRpTJUGhklDv"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81w1lahhkozO"
      },
      "source": [
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.BCELoss(),\n",
        "          train_loader = train_iter,\n",
        "          valid_loader = valid_iter,\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_iter) // 2,\n",
        "          file_path = destination_folder,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (labels, inputfield, desc), _ in train_loader:\n",
        "            labels = labels.type(torch.LongTensor)           \n",
        "            labels = labels.to(device)\n",
        "            desc = desc.type(torch.LongTensor)  \n",
        "            desc = desc.to(device)\n",
        "            output = model(desc, labels)\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for (labels, inputfield, desc), _ in valid_loader:\n",
        "                        labels = labels.type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        desc = desc.type(torch.LongTensor)  \n",
        "                        desc = desc.to(device)\n",
        "                        output = model(desc, labels)\n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHdi_cyEvC9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "11756896-1c4c-4083-ddc9-aa0e3dad2184"
      },
      "source": [
        "model = BERT().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "train(model=model, optimizer=optimizer)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-e4474bff9c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N1dBUZK5WCG"
      },
      "source": [
        "BERT()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JaIsEttvMeO"
      },
      "source": [
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qLyO8EWwJEo"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFBhAW6rwKly"
      },
      "source": [
        "# Evaluation Function\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (labels, title, text, titletext), _ in test_loader:\n",
        "\n",
        "                labels = labels.type(torch.LongTensor)           \n",
        "                labels = labels.to(device)\n",
        "                titletext = titletext.type(torch.LongTensor)  \n",
        "                titletext = titletext.to(device)\n",
        "                output = model(titletext, labels)\n",
        "\n",
        "                _, output = output\n",
        "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
        "                y_true.extend(labels.tolist())\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n",
        "    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLlijSLcxv2j"
      },
      "source": [
        "best_model = BERT().to(device)\n",
        "\n",
        "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
        "\n",
        "evaluate(best_model, test_iter)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}